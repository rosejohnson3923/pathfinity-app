/**
 * AI Service Selector
 * Conditionally uses multi-model system or standard Azure OpenAI based on configuration
 */

import { azureOpenAIService } from '../azureOpenAIService';
import { MultiModelService } from './MultiModelService';
import { ModelRouter } from './ModelRouter';
import { getModelForGrade } from './ModelCapabilities';
import * as fs from 'fs';
import * as dotenv from 'dotenv';

// Load multi-model configuration if available
if (typeof window === 'undefined' && fs.existsSync && fs.existsSync('.env.multimodel')) {
  dotenv.config({ path: '.env.multimodel' });
}

export interface AIServiceInterface {
  generateContent(params: ContentGenerationParams): Promise<any>;
  generateAssessmentContent?(params: ContentGenerationParams): Promise<any>;
}

export interface ContentGenerationParams {
  prompt: string;
  context?: {
    grade?: string;
    subject?: string;
    skill?: string;
    studentProfile?: any;
  };
  maxTokens?: number;
  temperature?: number;
}

class AIServiceSelector implements AIServiceInterface {
  private multiModelService: MultiModelService | null = null;
  private useMultiModel: boolean = false;
  private targetGrades: string[] = [];
  private debugMode: boolean = false;

  constructor() {
    this.initialize();
  }

  private initialize() {
    // Check if multi-model is enabled
    this.useMultiModel = process.env.ENABLE_MULTI_MODEL === 'true';

    // Get target grades for rollout
    if (process.env.MULTI_MODEL_TARGET_GRADES) {
      this.targetGrades = process.env.MULTI_MODEL_TARGET_GRADES.split(',').map(g => g.trim());
    }

    // Debug mode
    this.debugMode = process.env.MULTI_MODEL_DEBUG === 'true';

    // Initialize multi-model service if enabled
    if (this.useMultiModel) {
      try {
        this.multiModelService = new MultiModelService();
        console.log('');
        console.log('â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—');
        console.log('â•‘        ğŸš€ MULTI-MODEL AI SYSTEM INITIALIZED ğŸš€            â•‘');
        console.log('â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£');
        console.log('â•‘ Status: ACTIVE                                            â•‘');
        console.log(`â•‘ Target Grades: ${this.targetGrades.join(', ').padEnd(43)}â•‘`);
        console.log(`â•‘ Debug Mode: ${this.debugMode ? 'ON' : 'OFF'}                                          â•‘`);
        console.log('â•‘ Cost Savings: 81.6% vs GPT-4o                            â•‘');
        console.log('â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');
        console.log('');
      } catch (error) {
        console.error('âŒ Failed to initialize multi-model service:', error);
        this.useMultiModel = false;
      }
    }
  }

  /**
   * Determines if we should use multi-model for this request
   */
  private shouldUseMultiModel(params: ContentGenerationParams): boolean {
    if (!this.useMultiModel || !this.multiModelService) {
      return false;
    }

    // Check if grade is in target grades for rollout
    const grade = params.context?.grade;
    if (grade && this.targetGrades.length > 0) {
      const shouldUse = this.targetGrades.includes(grade);

      console.log('');
      console.log('â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”');
      console.log('â”‚      ğŸ¯ MODEL SELECTION DECISION        â”‚');
      console.log('â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤');
      console.log(`â”‚ Grade: ${grade.padEnd(33)}â”‚`);
      console.log(`â”‚ Subject: ${(params.context?.subject || 'N/A').padEnd(31)}â”‚`);
      console.log(`â”‚ Skill: ${(params.context?.skill || 'N/A').substring(0, 31).padEnd(33)}â”‚`);
      console.log('â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤');
      console.log(`â”‚ Decision: ${shouldUse ? 'âœ… MULTI-MODEL' : 'â­ï¸  STANDARD GPT-4o'}          â”‚`);
      console.log('â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜');

      return shouldUse;
    }

    // Check rollout percentage
    const rolloutPercentage = parseInt(process.env.MULTI_MODEL_ROLLOUT_PERCENTAGE || '0');
    if (rolloutPercentage > 0 && rolloutPercentage < 100) {
      const random = Math.random() * 100;
      return random < rolloutPercentage;
    }

    return this.useMultiModel;
  }

  /**
   * Main content generation method
   */
  async generateContent(params: ContentGenerationParams): Promise<any> {
    const startTime = Date.now();

    try {
      // Determine which service to use
      if (this.shouldUseMultiModel(params)) {
        // Use multi-model system
        const result = await this.generateWithMultiModel(params);

        const latency = Date.now() - startTime;
        console.log('');
        console.log('â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—');
        console.log('â•‘     âœ… MULTI-MODEL RESPONSE COMPLETE      â•‘');
        console.log('â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£');
        console.log(`â•‘ Model Used: ${(result.modelUsed || 'Unknown').padEnd(30)}â•‘`);
        console.log(`â•‘ Latency: ${(latency + 'ms').padEnd(33)}â•‘`);
        console.log(`â•‘ Cost: $${(result.cost?.toFixed(6) || '0.000000').padEnd(35)}â•‘`);
        console.log(`â•‘ Tokens: ${((result.tokens || 0) + ' total').padEnd(34)}â•‘`);
        console.log('â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');

        return result.content;
      } else {
        // Use standard Azure OpenAI service
        const result = await azureOpenAIService.generateContent(
          params.prompt,
          params.maxTokens || 2000,
          params.temperature || 0.7
        );

        const latency = Date.now() - startTime;
        console.log('');
        console.log('â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”');
        console.log('â”‚   â­ï¸  STANDARD GPT-4o RESPONSE          â”‚');
        console.log('â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤');
        console.log(`â”‚ Latency: ${(latency + 'ms').padEnd(31)}â”‚`);
        console.log(`â”‚ Est. Cost: $0.005000                   â”‚`);
        console.log('â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜');

        return result;
      }
    } catch (error) {
      console.error('AI Service Error:', error);

      // Fallback to standard service on error
      if (this.shouldUseMultiModel(params)) {
        console.log('Falling back to standard Azure OpenAI service');
        return azureOpenAIService.generateContent(
          params.prompt,
          params.maxTokens || 2000,
          params.temperature || 0.7
        );
      }

      throw error;
    }
  }

  /**
   * Generate content using multi-model system
   */
  private async generateWithMultiModel(params: ContentGenerationParams): Promise<any> {
    if (!this.multiModelService) {
      throw new Error('Multi-model service not initialized');
    }

    // Route the request to appropriate model
    const routingContext = {
      studentProfile: {
        grade: params.context?.grade,
        subject: params.context?.subject
      },
      promptType: 'content_generation' as const,
      content: params.prompt
    };

    const selectedModel = ModelRouter.routeRequest(routingContext);

    console.log('');
    console.log('â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”');
    console.log('â”‚       ğŸ“Š MODEL ROUTING RESULT           â”‚');
    console.log('â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤');
    console.log(`â”‚ Selected: ${selectedModel.name.padEnd(30)}â”‚`);
    console.log(`â”‚ Endpoint: ${selectedModel.endpoint?.substring(8, 30).padEnd(30)}â”‚`);
    console.log(`â”‚ Cost/1K: $${((selectedModel.costPerMilTokensInput || 0) / 1000).toFixed(6).padEnd(29)}â”‚`);
    console.log('â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜');

    // Generate content with selected model
    const result = await this.multiModelService.generateContent({
      grade: params.context?.grade || '5',
      subject: params.context?.subject || 'General',
      skill: params.context?.skill || '',
      contentType: 'learning_content',
      prompt: params.prompt
    });

    return result;
  }

  /**
   * Generate assessment content (specialized method)
   */
  async generateAssessmentContent(params: ContentGenerationParams): Promise<any> {
    // For assessments, we might want to use a more capable model
    // even for lower grades to ensure accuracy
    if (this.shouldUseMultiModel(params)) {
      const grade = params.context?.grade;
      const gradeNum = grade === 'K' ? 0 : parseInt(grade || '5');

      // For assessments, use a step up in model quality
      if (gradeNum <= 2) {
        // Use Llama instead of Phi-4 for assessments
        params.context = {
          ...params.context,
          grade: '3' // This will trigger Llama selection
        };
      }
    }

    return this.generateContent(params);
  }

  /**
   * Get cost estimate for a request
   */
  getCostEstimate(params: ContentGenerationParams): number {
    if (!this.shouldUseMultiModel(params)) {
      // GPT-4o estimate
      return 0.005; // Rough estimate
    }

    const grade = params.context?.grade || '5';
    const model = getModelForGrade(grade);

    if (!model) {
      return 0.005;
    }

    // Estimate: 500 input tokens, 200 output tokens
    const cost = (model.costPerMilTokensInput * 0.5 / 1000) +
                 (model.costPerMilTokensOutput * 0.2 / 1000);

    return cost;
  }

  /**
   * Get statistics about model usage
   */
  getStatistics() {
    return {
      multiModelEnabled: this.useMultiModel,
      targetGrades: this.targetGrades,
      rolloutPercentage: process.env.MULTI_MODEL_ROLLOUT_PERCENTAGE || '0',
      debugMode: this.debugMode
    };
  }
}

// Export singleton instance
export const aiServiceSelector = new AIServiceSelector();

// Export for backwards compatibility
export const generateAIContent = (params: ContentGenerationParams) => {
  return aiServiceSelector.generateContent(params);
};